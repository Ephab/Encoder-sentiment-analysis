{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "# !pip install -U \"transformers==4.44.2\" \"huggingface_hub==0.24.6\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9LdSpgOP2qI",
        "outputId": "daafdae0-05fa-41a3-8103-e73323c6464d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:17:52.091028Z",
          "iopub.execute_input": "2025-11-09T07:17:52.091358Z",
          "iopub.status.idle": "2025-11-09T07:18:08.718186Z",
          "shell.execute_reply.started": "2025-11-09T07:17:52.091323Z",
          "shell.execute_reply": "2025-11-09T07:18:08.717271Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torchinfo\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "cNEc3sSthXDK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:18:08.719754Z",
          "iopub.execute_input": "2025-11-09T07:18:08.719990Z",
          "iopub.status.idle": "2025-11-09T07:18:15.267627Z",
          "shell.execute_reply.started": "2025-11-09T07:18:08.719971Z",
          "shell.execute_reply": "2025-11-09T07:18:15.266811Z"
        }
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 200 # every sentence can only have 512 words max\n",
        "EMBEDDING_DIM = 104\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "RuE2zGWN4NWY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:18:15.268351Z",
          "iopub.execute_input": "2025-11-09T07:18:15.268671Z",
          "iopub.status.idle": "2025-11-09T07:18:16.005625Z",
          "shell.execute_reply.started": "2025-11-09T07:18:15.268653Z",
          "shell.execute_reply": "2025-11-09T07:18:16.004777Z"
        }
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "from datasets import Value, DatasetDict\n",
        "\n",
        "# IMDB: 50k long-form reviews\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "# GLUE SST-2: 67k short movie review phrases\n",
        "sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "sst2 = sst2.rename_column(\"sentence\", \"text\")\n",
        "\n",
        "# StanfordNLP SST2: ~70k phrases, slightly different split\n",
        "sst2_alt = load_dataset(\"stanfordnlp/sst2\")\n",
        "if \"sentence\" in sst2_alt[\"train\"].column_names:\n",
        "    sst2_alt = sst2_alt.rename_column(\"sentence\", \"text\")\n",
        "\n",
        "\n",
        "def ensure_int_label(example):\n",
        "    example[\"label\"] = int(example[\"label\"])\n",
        "    return example\n",
        "\n",
        "imdb = imdb.map(ensure_int_label)\n",
        "sst2 = sst2.map(ensure_int_label)\n",
        "sst2_alt = sst2_alt.map(ensure_int_label)\n",
        "\n",
        "def cast_int(ds):\n",
        "    return ds.cast_column(\"label\", Value(\"int64\"))\n",
        "\n",
        "if isinstance(imdb, DatasetDict):\n",
        "    imdb[\"train\"] = cast_int(imdb[\"train\"])\n",
        "    imdb[\"test\"] = cast_int(imdb[\"test\"])\n",
        "else:\n",
        "    imdb = cast_int(imdb)\n",
        "\n",
        "if isinstance(sst2, DatasetDict):\n",
        "    sst2[\"train\"] = cast_int(sst2[\"train\"])\n",
        "    sst2[\"validation\"] = cast_int(sst2[\"validation\"])\n",
        "else:\n",
        "    sst2 = cast_int(sst2)\n",
        "\n",
        "if isinstance(sst2_alt, DatasetDict):\n",
        "    sst2_alt[\"train\"] = cast_int(sst2_alt[\"train\"])\n",
        "    sst2_alt[\"validation\"] = cast_int(sst2_alt[\"validation\"])\n",
        "else:\n",
        "    sst2_alt = cast_int(sst2_alt)\n",
        "\n",
        "train_mix = concatenate_datasets([\n",
        "    imdb[\"train\"],\n",
        "    sst2[\"train\"],\n",
        "    sst2_alt[\"train\"],\n",
        "])\n",
        "\n",
        "test_mix = concatenate_datasets([\n",
        "    imdb[\"test\"],\n",
        "    sst2[\"validation\"],\n",
        "    sst2_alt[\"validation\"],\n",
        "])\n",
        "\n",
        "train_mix = train_mix.shuffle(seed=42)\n",
        "test_mix = test_mix.shuffle(seed=42)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_mix,\n",
        "    \"test\": test_mix\n",
        "})"
      ],
      "metadata": {
        "id": "dyTvxr9SSJZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15T4UI5RSvm7",
        "outputId": "4ee65d41-2838-4090-ef2e-09790f7d2de1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'idx'],\n",
              "        num_rows: 159698\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'idx'],\n",
              "        num_rows: 26744\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(example):\n",
        "    return tokenizer(example['text'],\n",
        "                     padding=\"max_length\",\n",
        "                     max_length=MAX_LENGTH,\n",
        "                     truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n",
        "tokenized_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "KpJPMyPwCgQ2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:18:19.709303Z",
          "iopub.execute_input": "2025-11-09T07:18:19.709677Z",
          "iopub.status.idle": "2025-11-09T07:19:52.101485Z",
          "shell.execute_reply.started": "2025-11-09T07:18:19.709650Z",
          "shell.execute_reply": "2025-11-09T07:19:52.100914Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = tokenized_dataset['train']\n",
        "tokenized_test = tokenized_dataset['test']\n",
        "\n",
        "tokenized_train, tokenized_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3eh53hl3IaE",
        "outputId": "3cb7fc11-89f7-4707-d51a-e6df58283584",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:52.102158Z",
          "iopub.execute_input": "2025-11-09T07:19:52.102377Z",
          "iopub.status.idle": "2025-11-09T07:19:52.107256Z",
          "shell.execute_reply.started": "2025-11-09T07:19:52.102360Z",
          "shell.execute_reply": "2025-11-09T07:19:52.106582Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['text', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "     num_rows: 159698\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['text', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "     num_rows: 26744\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "# temp\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "a08FhxpkV-R8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=tokenized_train, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=tokenized_test, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "sHLffRbcCrEv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:52.108037Z",
          "iopub.execute_input": "2025-11-09T07:19:52.108284Z",
          "iopub.status.idle": "2025-11-09T07:19:52.216663Z",
          "shell.execute_reply.started": "2025-11-09T07:19:52.108262Z",
          "shell.execute_reply": "2025-11-09T07:19:52.216010Z"
        }
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aN_JAFhB4CPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, sequence_length, embedding_dim):\n",
        "        super().__init__()\n",
        "        position_matrix = torch.zeros((sequence_length, embedding_dim))\n",
        "\n",
        "        rows = torch.arange(0, sequence_length).reshape(-1, 1)\n",
        "        even_columns = torch.arange(0, embedding_dim, 2)\n",
        "\n",
        "        denominator = torch.pow(10000, (even_columns.float() / embedding_dim))\n",
        "\n",
        "        entries = rows/denominator\n",
        "\n",
        "        position_matrix[:, ::2] = torch.sin(entries)\n",
        "        position_matrix[:, 1::2] = torch.cos(entries)\n",
        "\n",
        "        self.register_buffer('position_matrix', position_matrix)\n",
        "\n",
        "    def forward(self, word_embedding):\n",
        "        x_shape = word_embedding.shape #[batch, seq, embed_dim]\n",
        "\n",
        "        pos = self.position_matrix[:x_shape[1], :]\n",
        "\n",
        "        return word_embedding + pos"
      ],
      "metadata": {
        "id": "eGIzBcqWkILT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:52.217419Z",
          "iopub.execute_input": "2025-11-09T07:19:52.217695Z",
          "iopub.status.idle": "2025-11-09T07:19:52.229662Z",
          "shell.execute_reply.started": "2025-11-09T07:19:52.217671Z",
          "shell.execute_reply": "2025-11-09T07:19:52.229128Z"
        }
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, sequence_length):\n",
        "        super().__init__()\n",
        "        #num_embeddings is how many entries we have in the embedding matrix\n",
        "        #embedding_dim is how long each embedding vector is in the embedding matrix\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "        #positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(sequence_length=sequence_length, embedding_dim=embedding_dim)\n",
        "\n",
        "        #encoder layer\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=2048, batch_first=True)\n",
        "\n",
        "        self.encoder_stack = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
        "\n",
        "        self.classifier = nn.Linear(in_features=embedding_dim, out_features=2) #takes in embedding dim, and outputs 2 choices (+, -)\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        #get the word embeddings for the ids\n",
        "        embeddings = self.embedding_layer(x)\n",
        "\n",
        "        #add the positional encoding to the word embeddings\n",
        "        embeddings_plus_position = self.positional_encoding(embeddings)\n",
        "\n",
        "\n",
        "        # encoding = self.encoder_stack(embeddings_plus_position, src_key_padding_mask=~padding_mask)\n",
        "        encoding = self.encoder_stack(embeddings_plus_position, src_key_padding_mask=torch.logical_not(padding_mask))\n",
        "        # The output is now [batch_size, seq_length, embedding_dim]\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        CLS_token = self.dropout(encoding[:,0,:]) #we want the first token (The CLS token which is just the summary of all the other context rich vectors in the sentence)\n",
        "        # So now we have [batch_size, embedding_dim]\n",
        "\n",
        "        output = self.classifier(CLS_token)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "QbX65q2dhkKu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:52.231908Z",
          "iopub.execute_input": "2025-11-09T07:19:52.232139Z",
          "iopub.status.idle": "2025-11-09T07:19:52.243138Z",
          "shell.execute_reply.started": "2025-11-09T07:19:52.232125Z",
          "shell.execute_reply": "2025-11-09T07:19:52.242467Z"
        }
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Training Loop\n",
        "\n"
      ],
      "metadata": {
        "id": "g9ipaKng4R39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=EMBEDDING_DIM, sequence_length=MAX_LENGTH).to(device)\n",
        "model = torch.compile(model)"
      ],
      "metadata": {
        "id": "Qb9YZhxIHzi7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:52.243881Z",
          "iopub.execute_input": "2025-11-09T07:19:52.244383Z",
          "iopub.status.idle": "2025-11-09T07:19:58.565098Z",
          "shell.execute_reply.started": "2025-11-09T07:19:52.244367Z",
          "shell.execute_reply": "2025-11-09T07:19:58.564262Z"
        }
      },
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw9Lx8hEPz01",
        "outputId": "8ca9bef9-f724-4511-a79e-ff7bf5410afd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:58.566247Z",
          "iopub.execute_input": "2025-11-09T07:19:58.566708Z",
          "iopub.status.idle": "2025-11-09T07:19:58.577932Z",
          "shell.execute_reply.started": "2025-11-09T07:19:58.566681Z",
          "shell.execute_reply": "2025-11-09T07:19:58.577274Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                                                 Param #\n",
              "===============================================================================================\n",
              "OptimizedModule                                                        --\n",
              "├─TextClassifier: 1-1                                                  --\n",
              "│    └─Embedding: 2-1                                                  3,174,288\n",
              "│    └─PositionalEncoding: 2-2                                         --\n",
              "│    └─TransformerEncoderLayer: 2-3                                    --\n",
              "│    │    └─MultiheadAttention: 3-1                                    43,680\n",
              "│    │    └─Linear: 3-2                                                215,040\n",
              "│    │    └─Dropout: 3-3                                               --\n",
              "│    │    └─Linear: 3-4                                                213,096\n",
              "│    │    └─LayerNorm: 3-5                                             208\n",
              "│    │    └─LayerNorm: 3-6                                             208\n",
              "│    │    └─Dropout: 3-7                                               --\n",
              "│    │    └─Dropout: 3-8                                               --\n",
              "│    └─TransformerEncoder: 2-4                                         --\n",
              "│    │    └─ModuleList: 3-9                                            2,833,392\n",
              "│    └─Linear: 2-5                                                     210\n",
              "===============================================================================================\n",
              "Total params: 6,480,122\n",
              "Trainable params: 6,480,122\n",
              "Non-trainable params: 0\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.005)"
      ],
      "metadata": {
        "id": "CL3YEC-PHexQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:58.578879Z",
          "iopub.execute_input": "2025-11-09T07:19:58.579145Z",
          "iopub.status.idle": "2025-11-09T07:19:58.595523Z",
          "shell.execute_reply.started": "2025-11-09T07:19:58.579123Z",
          "shell.execute_reply": "2025-11-09T07:19:58.594730Z"
        }
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            X = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].bool().to(device)\n",
        "            y = batch['label'].to(device)\n",
        "\n",
        "            output = model(X, mask)\n",
        "            total_correct += (torch.argmax(output,1) == y).sum().item()\n",
        "    model.train()\n",
        "    return total_correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "IRA920fvMqs5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:58.596267Z",
          "iopub.execute_input": "2025-11-09T07:19:58.596509Z",
          "iopub.status.idle": "2025-11-09T07:19:58.611083Z",
          "shell.execute_reply.started": "2025-11-09T07:19:58.596488Z",
          "shell.execute_reply": "2025-11-09T07:19:58.610345Z"
        }
      },
      "outputs": [],
      "execution_count": 44
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            X = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].bool().to(device)\n",
        "            y = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                output = model(X, mask)\n",
        "                loss = loss_fn(output, y)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            scaler.update()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}   Training Loss= {epoch_loss/len(train_loader)} \", end=\"\")\n",
        "        print(f\"   Train Accuracy={get_accuracy(train_loader)}   Test Accuracy= {get_accuracy(test_loader)}\" if (epoch+1) % 5 == 0 else \"\")"
      ],
      "metadata": {
        "id": "5aN2mI0ahrRY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:19:58.611831Z",
          "iopub.execute_input": "2025-11-09T07:19:58.612036Z",
          "iopub.status.idle": "2025-11-09T07:19:58.624016Z",
          "shell.execute_reply.started": "2025-11-09T07:19:58.612021Z",
          "shell.execute_reply": "2025-11-09T07:19:58.623279Z"
        }
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "train(10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:37:28.312917Z",
          "iopub.execute_input": "2025-11-09T07:37:28.313275Z",
          "iopub.status.idle": "2025-11-09T07:43:12.803778Z",
          "shell.execute_reply.started": "2025-11-09T07:37:28.313249Z",
          "shell.execute_reply": "2025-11-09T07:43:12.802496Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBs27qKjPpQ3",
        "outputId": "e83788cb-f062-4695-e8f4-9f58a603a41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1   Training Loss= 0.5100580325397925 \n",
            "Epoch 2   Training Loss= 0.31759124085641444 \n",
            "Epoch 3   Training Loss= 0.2432815367833544 \n",
            "Epoch 4   Training Loss= 0.19838611737419015 \n",
            "Epoch 5   Training Loss= 0.16585937391545696    Train Accuracy=0.9573883204548586   Test Accuracy= 0.8100508525276697\n",
            "Epoch 6   Training Loss= 0.14238897847751966 \n",
            "Epoch 7   Training Loss= 0.12468794043748997 \n",
            "Epoch 8   Training Loss= 0.11054382047045809 \n",
            "Epoch 9   Training Loss= 0.10064033838436724 \n",
            "Epoch 10   Training Loss= 0.0900825643270181    Train Accuracy=0.9817843679945898   Test Accuracy= 0.8195857014657493\n"
          ]
        }
      ],
      "execution_count": 46
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This movie was not so bad!\"\n",
        "inputs = tokenizer(text, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(inputs['input_ids'], inputs['attention_mask'].bool())\n",
        "    pred = torch.argmax(output, dim=1).item()\n",
        "\n",
        "print(\"Prediction:\", \"Positive\" if pred == 1 else \"Negative\", output)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:43:16.960857Z",
          "iopub.execute_input": "2025-11-09T07:43:16.961625Z",
          "iopub.status.idle": "2025-11-09T07:43:17.024131Z",
          "shell.execute_reply.started": "2025-11-09T07:43:16.961598Z",
          "shell.execute_reply": "2025-11-09T07:43:17.023409Z"
        },
        "id": "ri6XSiNpPpQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd51b64d-240e-4f31-fc4f-82bd342498d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Positive tensor([[0.0050, 2.1282]], device='cuda:0')\n"
          ]
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_phrases = [\n",
        "    \"This movie was great!\",\n",
        "    \"This movie was terrible!\",\n",
        "    \"This movie was not great.\",\n",
        "    \"This movie was not terrible.\",\n",
        "    \"This movie was not bad at all!\",\n",
        "    \"This is so good!\",\n",
        "    \"This is not so good.\",\n",
        "    \"It's so good I watched it twice.\",\n",
        "    \"It's not so good, honestly.\",\n",
        "    \"Okay but not so great.\",\n",
        "    \"Surprisingly not so bad!\"\n",
        "]\n",
        "\n",
        "for t in test_phrases:\n",
        "    inputs = tokenizer(t, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'].bool())\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred = \"Positive\" if probs[1] > 0.5 else \"Negative\"\n",
        "    print(f\"{t:<45} → {pred}  |  pos: {probs[1]:.3f}  neg: {probs[0]:.3f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T07:43:22.535029Z",
          "iopub.execute_input": "2025-11-09T07:43:22.535685Z",
          "iopub.status.idle": "2025-11-09T07:43:22.595139Z",
          "shell.execute_reply.started": "2025-11-09T07:43:22.535663Z",
          "shell.execute_reply": "2025-11-09T07:43:22.594476Z"
        },
        "id": "NnZLxpB9PpQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7631b7f5-810e-443f-8b76-0d1942649d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie was great!                         → Positive  |  pos: 1.000  neg: 0.000\n",
            "This movie was terrible!                      → Negative  |  pos: 0.000  neg: 1.000\n",
            "This movie was not great.                     → Negative  |  pos: 0.008  neg: 0.992\n",
            "This movie was not terrible.                  → Negative  |  pos: 0.004  neg: 0.996\n",
            "This movie was not bad at all!                → Positive  |  pos: 0.917  neg: 0.083\n",
            "This is so good!                              → Positive  |  pos: 0.999  neg: 0.001\n",
            "This is not so good.                          → Negative  |  pos: 0.001  neg: 0.999\n",
            "It's so good I watched it twice.              → Positive  |  pos: 0.999  neg: 0.001\n",
            "It's not so good, honestly.                   → Negative  |  pos: 0.001  neg: 0.999\n",
            "Okay but not so great.                        → Negative  |  pos: 0.079  neg: 0.921\n",
            "Surprisingly not so bad!                      → Positive  |  pos: 0.981  neg: 0.019\n"
          ]
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in model.state_dict().items()}\n",
        "\n",
        "inference_bundle = {\n",
        "    \"model_state_dict\": state_dict,\n",
        "    \"config\": {\n",
        "        \"tokenizer_name\": \"bert-base-uncased\",\n",
        "        \"max_len\": MAX_LENGTH,\n",
        "        \"embedding_dim\": EMBEDDING_DIM,\n",
        "        \"vocab_size\": tokenizer.vocab_size,\n",
        "        \"num_heads\": 8,\n",
        "        \"num_layers\": 6,\n",
        "        \"ff_dim\": 2048,\n",
        "        \"classes\": [\"Negative\", \"Positive\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "save_path = \"custom_encoder_portable_model.pth\"\n",
        "torch.save(inference_bundle, save_path)\n",
        "\n",
        "print(f\"Saved portable model to: {save_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "blDLAHuWPpQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1409bb34-4ab4-4d27-c9ee-41f5b93c55f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved portable model to: custom_encoder_portable_model.pth\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DY52EBOJ68nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}