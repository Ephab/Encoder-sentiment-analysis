{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ1mgw/yxO4wum26o/gETb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tm5zOts04TFa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# POSitional Encoding\n",
        "# ==========================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, sequence_length, embedding_dim):\n",
        "        super().__init__()\n",
        "        position_matrix = torch.zeros((sequence_length, embedding_dim))\n",
        "\n",
        "        rows = torch.arange(0, sequence_length).reshape(-1, 1)\n",
        "        even_columns = torch.arange(0, embedding_dim, 2)\n",
        "        denominator = torch.pow(10000, even_columns.float() / embedding_dim)\n",
        "        entries = rows / denominator\n",
        "\n",
        "        position_matrix[:, ::2] = torch.sin(entries)\n",
        "        position_matrix[:, 1::2] = torch.cos(entries)\n",
        "\n",
        "        self.register_buffer(\"position_matrix\", position_matrix)\n",
        "\n",
        "    def forward(self, word_embedding):\n",
        "        # word_embedding: [batch, seq, embed_dim]\n",
        "        seq_len = word_embedding.size(1)\n",
        "        return word_embedding + self.position_matrix[:seq_len, :]\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# TEXT CLASSIFIER\n",
        "# ==========================================\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, sequence_length,\n",
        "                 nhead=8, ff_dim=2048, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            sequence_length=sequence_length,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=ff_dim,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # encoder_stack built from encoder_layer (same as training)\n",
        "        self.encoder_stack = nn.TransformerEncoder(\n",
        "            self.encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=2\n",
        "        )\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        # x: [batch, seq]\n",
        "        embeddings = self.embedding_layer(x)\n",
        "        embeddings_plus_position = self.positional_encoding(embeddings)\n",
        "\n",
        "        # padding_mask: True = real token, False = pad\n",
        "        src_key_padding_mask = torch.logical_not(padding_mask)\n",
        "\n",
        "        encoding = self.encoder_stack(\n",
        "            embeddings_plus_position,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        cls_token = encoding[:, 0, :]  # [batch, embed_dim]\n",
        "        output = self.classifier(cls_token)\n",
        "        return output\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# SENTIMENT PREDICTOR\n",
        "# ==========================================\n",
        "class SentimentTransformer:\n",
        "    def __init__(self, model_path=\"custom_encoder_portable_model.pth\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Loading model on: {self.device}\")\n",
        "\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "        cfg = checkpoint[\"config\"]\n",
        "\n",
        "        self.config = cfg\n",
        "        self.classes = cfg[\"classes\"]\n",
        "        self.max_len = cfg[\"max_len\"]\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(cfg[\"tokenizer_name\"])\n",
        "\n",
        "        # rebuild model EXACTLY like during training\n",
        "        self.model = TextClassifier(\n",
        "            vocab_size=cfg[\"vocab_size\"],\n",
        "            embedding_dim=cfg[\"embedding_dim\"],\n",
        "            sequence_length=cfg[\"max_len\"],\n",
        "            nhead=cfg.get(\"num_heads\", 8),\n",
        "            ff_dim=cfg.get(\"ff_dim\", 2048),\n",
        "            num_layers=cfg.get(\"num_layers\", 6),\n",
        "        ).to(self.device)\n",
        "\n",
        "        # load weights\n",
        "        state_dict = {\n",
        "            k.replace(\"_orig_mod.\", \"\"): v\n",
        "            for k, v in checkpoint[\"model_state_dict\"].items()\n",
        "        }\n",
        "        self.model.load_state_dict(state_dict)  # strict=True by default\n",
        "\n",
        "        self.model.eval()\n",
        "        print(\"Model loaded successfully.\")\n",
        "\n",
        "    def preprocess(self, text: str):\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {k: v.to(self.device) for k, v in encoded.items()}\n",
        "\n",
        "    def predict(self, text: str):\n",
        "        batch = self.preprocess(text)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(\n",
        "                batch[\"input_ids\"],\n",
        "                batch[\"attention_mask\"].bool(),\n",
        "            )\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            prediction_idx = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        label = self.classes[prediction_idx]\n",
        "        confidence = probs[0][prediction_idx].item()\n",
        "\n",
        "        print(f\"\"\"\n",
        "                Prediction:\n",
        "                    text:       {text}\n",
        "                    label:      {label}\n",
        "                    confidence: {confidence:.2%}\n",
        "                    probs:      {probs[0].cpu().numpy()}\n",
        "                \"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentTransformer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnItFBZm7DHb",
        "outputId": "9c779afc-1277-4344-976e-627cb373e868"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model on: cpu\n",
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(\"This is not a very good car at all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES0gDmKX7Vgc",
        "outputId": "b9e46a92-4222-4c00-88d2-8cd6cba3c271"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                Prediction:\n",
            "                    text:       This is not a very good car at all\n",
            "                    label:      Negative\n",
            "                    confidence: 99.95%\n",
            "                    probs:      [9.994585e-01 5.415360e-04]\n",
            "                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Grj8wYD9Eds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}